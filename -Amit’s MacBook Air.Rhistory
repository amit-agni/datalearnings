#This categorical needs to be handled separately
table(DT_train$ps_car_11_cat)
#TO DO : Assign levels before spliting, so that new levels can be handled in R
#TO DO : Spliting using MLR ? trainTestSplit <- makeResampleDesc("Holdout", split = 0.8,stratify.cols = "target")
#TO DO : Address the vtreat prepare warning
str(DT_train)
index <- caret::createDataPartition(DT_train$target, p = 0.005,list = FALSE)
train <- DT_train[index,]
test  <- DT_train[-index,]
#Encoding using vtreat
vars <- names(train)
treatplan <- designTreatmentsC(train,vars,outcomename = "target",outcometarget = 1,verbose = TRUE) #For Categoricals
newvars <- setDT(treatplan$scoreFrame)[code %in% c("clean","lev")]$varName
train <- prepare(treatplan,train,varRestriction = newvars) #Gives warning
str(train)
str(train)
train$target
embed_methods <- c("Isomap", "PCA")
dimRed::embed(train[1:100,])
dimRed::embed(train[1:100,], method ="Isomap")
train[,-c("target","id")]
## apply dimensionality reduction
data_emb <- lapply(embed_methods, function(x) embed(train[,!..c("target","id")], x))
## apply dimensionality reduction
cols <- c("target","id")
train[,!..cols]
## apply dimensionality reduction
cols <- c("target","id")
train[,!..cols]
str(train)
setDT(train)
## apply dimensionality reduction
cols <- c("target","id")
data_emb <- lapply(embed_methods, function(x) embed(train[1:100,!..cols], x))
install.packages('RSpectra')
data_emb <- lapply(embed_methods, function(x) embed(train[1:100,!..cols], x))
install.packages('RANN')
data_emb <- lapply(embed_methods, function(x) embed(train[1:100,!..cols], x))
data_emb <- lapply(embed_methods, function(x) embed(train[,!..cols], x))
names(data_emb) <- embed_methods
plot(data_set, type = "3vars")
plot(train, type = "3vars")
lapply(data_emb, plot, type = "2vars")
plot_R_NX(data_emb)
install.packages('coRanking')
plot_R_NX(data_emb)
data_emb$Isomap
plot(data_emb$Isomap)
lapply(data_emb,dimRed::plot, type = "2vars")
lapply(data_emb,dimRed::plot, type = "3vars")
lapply(data_emb,dimRed::plot, type = "pairs")
dimRed::dimRedMethodList()
index <- caret::createDataPartition(DT_train$target, p = 0.8,list = FALSE)
train <- DT_train[index,]
if(!require(cutlery)) { devtools::install_github("amit-agni/cutlery"); library(cutlery)}
if(!require(pacman)) { install.packages("pacman"); library(pacman)}
p_load(here,data.table,tidyverse,tictoc,kableExtra,gridExtra,RColorBrewer,DescTools
,na.tools # For basic NA imputation
,psych # for phi coefficient of correlation
,visdat # Missing value visualisation
,dimRed #dimRed
,mlr
,caret #stratified sampling only
,vtreat
)
source(here::here("content","R_scripts_and_functions","porto_seguro","porto_functions.R"))
root <- '/Mac Backup/R-Large-Items/porto/'
DT_train <- fread(file=paste(root,'train.csv',sep=''))
DT_train <- fn_baseCleanup(DT_train)
#This categorical needs to be handled separately
table(DT_train$ps_car_11_cat)
#TO DO : Assign levels before spliting, so that new levels can be handled in R
#TO DO : Spliting using MLR ? trainTestSplit <- makeResampleDesc("Holdout", split = 0.8,stratify.cols = "target")
#TO DO : Address the vtreat prepare warning
str(DT_train)
index <- caret::createDataPartition(DT_train$target, p = 0.8,list = FALSE)
train <- DT_train[index,]
test  <- DT_train[-index,]
#Encoding using vtreat
vars <- names(train)
treatplan <- designTreatmentsC(train,vars,outcomename = "target",outcometarget = 1,verbose = TRUE) #For Categoricals
newvars <- setDT(treatplan$scoreFrame)[code %in% c("clean","lev")]$varName
train <- prepare(treatplan,train,varRestriction = newvars) #Gives warning
#install.packages('RSpectra')
#install.packages('RANN')
#install.packages('coRanking')
str(train)
setDT(train)
embed_methods <- c("Isomap", "PCA") ## define which methods to apply
embed_methods <- dimRed::dimRedMethodList()
## apply dimensionality reduction
cols <- c("target","id")
data_emb <- lapply(embed_methods, function(x) embed(train[,!..cols], x))
install.packages('tensorflow')
plot_R_NX(data_emb)
data_emb <- lapply(embed_methods, function(x) embed(train[,!..cols], x))
Y
data_emb
embed_methods
embed_methods <- embed_methods[embed_methods != "autoencoder"]
data_emb <- lapply(embed_methods, function(x) embed(train[,!..cols], x))
embed_methods
embed_methods <- dimRed::dimRedMethodList()
embed_methods <- embed_methods[embed_methods != "Autoencoder"]
embed_methods
embed_methods <- embed_methods[embed_methods != "AutoEncoder"]
embed_methods
data_emb <- lapply(embed_methods, function(x) embed(train[,!..cols], x))
install.packages(dimRed,dependencies = c("Depends", "Imports", "LinkingTo", "Suggests", "Enhances"))
install.packages("dimRed",dependencies = c("Depends", "Imports", "LinkingTo", "Suggests", "Enhances"))
install.packages("dimRed",dependencies = c("Depends", "Imports", "LinkingTo", "Suggests", "Enhances"))
install.packages('diffusionMap')
if(!require(cutlery)) { devtools::install_github("amit-agni/cutlery"); library(cutlery)}
if(!require(pacman)) { install.packages("pacman"); library(pacman)}
p_load(here,data.table,tidyverse,tictoc,kableExtra,gridExtra,RColorBrewer,DescTools
,na.tools # For basic NA imputation
,psych # for phi coefficient of correlation
,visdat # Missing value visualisation
,dimRed #dimRed
,mlr
,caret #stratified sampling only
,vtreat
)
source(here::here("content","R_scripts_and_functions","porto_seguro","porto_functions.R"))
root <- '/Mac Backup/R-Large-Items/porto/'
DT_train <- fread(file=paste(root,'train.csv',sep=''))
DT_train <- fn_baseCleanup(DT_train)
#This categorical needs to be handled separately
table(DT_train$ps_car_11_cat)
#TO DO : Assign levels before spliting, so that new levels can be handled in R
#TO DO : Spliting using MLR ? trainTestSplit <- makeResampleDesc("Holdout", split = 0.8,stratify.cols = "target")
#TO DO : Address the vtreat prepare warning
str(DT_train)
index <- caret::createDataPartition(DT_train$target, p = 1,list = FALSE)
train <- DT_train[index,]
test  <- DT_train[-index,]
#Encoding using vtreat
vars <- names(train)
treatplan <- designTreatmentsC(train,vars,outcomename = "target",outcometarget = 1,verbose = TRUE) #For Categoricals
newvars <- setDT(treatplan$scoreFrame)[code %in% c("clean","lev")]$varName
train <- prepare(treatplan,train,varRestriction = newvars) #Gives warning
embed_methods <- c("LLE", "PCA","tSNE","FastICA","DRR") ## define which methods to apply
here::here()
str(train)
setDT(train)
saveRDS(train, here::here("content","R_scripts_and_functions","porto_seguro","train_vtreated.Rds"))
tic()
embed_methods <- c("LLE", "PCA","tSNE","FastICA","DRR") ## define which methods to apply
#embed_methods <- dimRed::dimRedMethodList()
#embed_methods <- embed_methods[embed_methods != "AutoEncoder"]
## apply dimensionality reduction
cols <- c("target","id")
data_emb <- lapply(embed_methods, function(x) embed(train[,!..cols], x))
names(data_emb) <- embed_methods
toc()
#install.packages('RSpectra')
#install.packages('RANN')
#install.packages('coRanking')
#install.packages('tensorflow')
#install.packages('diffusionMap')
install.packages('lle')
tic()
embed_methods <- c("LLE", "PCA","tSNE","FastICA","DRR") ## define which methods to apply
#embed_methods <- dimRed::dimRedMethodList()
#embed_methods <- embed_methods[embed_methods != "AutoEncoder"]
## apply dimensionality reduction
cols <- c("target","id")
data_emb <- lapply(embed_methods, function(x) embed(train[,!..cols], x))
names(data_emb) <- embed_methods
toc()
tic()
embed_methods <- c("LLE", "PCA","tSNE","FastICA","DRR") ## define which methods to apply
#embed_methods <- dimRed::dimRedMethodList()
#embed_methods <- embed_methods[embed_methods != "AutoEncoder"]
## apply dimensionality reduction
cols <- c("target","id")
data_emb <- lapply(embed_methods, function(x) embed(train[,!..cols], x))
names(data_emb) <- embed_methods
toc()
train <- readRDS(here::here("content","R_scripts_and_functions","porto_seguro","train_vtreated.Rds"))
if(!require(cutlery)) { devtools::install_github("amit-agni/cutlery"); library(cutlery)}
if(!require(pacman)) { install.packages("pacman"); library(pacman)}
p_load(here,data.table,tidyverse,tictoc,kableExtra,gridExtra,RColorBrewer,DescTools
,na.tools # For basic NA imputation
,psych # for phi coefficient of correlation
,visdat # Missing value visualisation
,dimRed #dimRed
,mlr
,caret #stratified sampling only
,vtreat
)
gc()
gc()
embed_methods <- c("LLE", "PCA") ## define which methods to apply
## apply dimensionality reduction
cols <- c("target","id")
data_emb <- lapply(embed_methods, function(x) embed(train[,!..cols], x))
if(!require(cutlery)) { devtools::install_github("amit-agni/cutlery"); library(cutlery)}
if(!require(pacman)) { install.packages("pacman"); library(pacman)}
p_load(here,data.table,tidyverse,tictoc,kableExtra,gridExtra,RColorBrewer,DescTools
,na.tools # For basic NA imputation
,psych # for phi coefficient of correlation
,visdat # Missing value visualisation
,dimRed #dimRed
,mlr
,caret #stratified sampling only
,vtreat
)
source(here::here("content","R_scripts_and_functions","porto_seguro","porto_functions.R"))
root <- '/Mac Backup/R-Large-Items/porto/'
DT_train <- fread(file=paste(root,'train.csv',sep=''))
DT_train <- fn_baseCleanup(DT_train)
str(DT_train)
fn_baseCleanup <- function(DT){
#Ignore the categorical NAs
#cols_NA_cat <- c("ps_ind_02_cat","ps_ind_04_cat","ps_ind_05_cat","ps_car_01_cat","ps_car_02_cat","ps_car_03_cat","ps_car_05_cat","ps_car_07_cat","ps_car_09_cat","ps_car_11")
#DT_train[,(cols_NA_cat) := lapply(.SD,na.tools::na.most_freq), .SDcols = cols_NA_cat ]
cols_NA_cont <- c("ps_car_14","ps_reg_03","ps_car_12")
DT[,(cols_NA_cont) := lapply(.SD,na.tools::na.median), .SDcols = cols_NA_cont ]
props <- lapply(DT,table)
#Convert all features with levels < 30 to factors
le30vars <- names(which(lapply(props,length)<30))
#With cat/bin features to nomimal
le30vars_nom <-grep("cat|bin",le30vars,value = T)
DT[,(le30vars_nom) := lapply(.SD,function(x) as.factor(as.character(x))),.SDcols = le30vars_nom]
#and rest to ordinal
# le30vars_ord <- le30vars[!le30vars %in% le30vars_nom]
# le30vars_ord <- le30vars_ord[le30vars_ord !="target"]
# DT[,(le30vars_ord) := lapply(.SD,function(x) as.ordered(as.character(x))),.SDcols = le30vars_ord]
DT
}
DT_train <- fread(file=paste(root,'train.csv',sep=''))
DT_train <- fn_baseCleanup(DT_train)
str(DT_train)
str(DT_train)
#Individual encodings
lapply(DT_train,nlevels)
#Individual encodings
lapply(DT_train,nlevels) > 2
#Individual encodings
lapply(DT_train,nlevels)[lapply(DT_train,nlevels) > 2]
#Individual encodings
bin_cols <- lapply(DT_train,nlevels)[lapply(DT_train,nlevels) == 2]
nonbin_cols <- lapply(DT_train,nlevels)[lapply(DT_train,nlevels) > 2]
p_load(FeatureHashing)
lapply(DT_train,nlevels)[lapply(DT_train,nlevels) > 2] %>% names()
nonbin_cols <- lapply(DT_train,nlevels)[lapply(DT_train,nlevels) > 2] %>% names()
#Individual encodings
bin_cols <- lapply(DT_train,nlevels)[lapply(DT_train,nlevels) == 2] %>% names()
train_hashed = hashed.model.matrix(~., data=DT_train[,nonbin_cols], hash.size=2^12, transpose=FALSE)
DT_train[,nonbin_cols]
train_hashed = hashed.model.matrix(~., data=DT_train[,..nonbin_cols], hash.size=2^12, transpose=FALSE)
train_hashed
train_hashed = as(train_hashed, "dgCMatrix")
train_hashed
DT_train[,..nonbin_cols]
nonbin_cols
nonbin_cols <- paste(nonbin_cols,"mean",sep="_")
nonbin_cols
nonbin_cols <- lapply(DT_train,nlevels)[lapply(DT_train,nlevels) > 2] %>% names()
n = 24L
set.seed(25)
DT <- data.table(
color = sample(c("green","yellow","red"), n, TRUE),
year = as.Date(sample(paste0(2011:2015,"-01-01"), n, TRUE)),
status = as.factor(sample(c("removed","active","inactive","archived"), n, TRUE)),
amount = sample(1:5, n, TRUE),
value = sample(c(3, 3.5, 2.5, 2), n, TRUE)
)
DT# rollup
# rollup
rollup(DT, j = sum(value), by = c("color","year","status")) # default id=FALSE
rollup(DT, j = sum(value), by = c("color","year","status"), id=TRUE)
rollup(DT, j = lapply(.SD, sum), by = c("color","year","status"), id=TRUE, .SDcols="value")
rollup(DT, j = c(list(count=.N), lapply(.SD, sum)), by = c("color","year","status"), id=TRUE)
# rollup
rollup(DT, j = sum(value), by = c("color","year","status")) # default id=FALSE
#mean encoding for nonbin cols
rollup(DT_train, j = mean(target), by = nonbin_cols) # default id=FALSE
# rollup
rollup(DT, j = sum(value), by = c("color","year","status")) # default id=FALSE
DT
groupingsets(DT, j = c(list(count=.N), lapply(.SD, sum)), by = c("color","year","status"),
sets = list("color", c("year","status"), character()), id=TRUE)
groupingsets(DT, j = c(list(count=.N), lapply(.SD, sum)), by = c("color","year","status"),
sets = list("color", "year","status"), character(), id=TRUE)
groupingsets(DT, j = c(list(count=.N), lapply(.SD, sum)), by = c("color","year","status"),
sets = list("color", "year","status"), id=F)
#mean encoding for nonbin cols
groupingsets(DT_train, j = mean(target), by = nonbin_cols,sets = list(nonbin_cols)) # default id=FALSE
nonbin_cols
list(nonbin_cols)
as.list(nonbin_cols)
#mean encoding for nonbin cols
groupingsets(DT_train, j = mean(target), by = nonbin_cols,sets = list(as.list(nonbin_cols))) # default id=FALSE
fn_agg <- function(DT,keycols_list,measure,measure_col) {
for(i in keycols_list) {
new_col_name <- paste0(paste0(unlist(i),collapse  ="_"),"_",eval(measure))
DT[,(new_col_name) := lapply(.SD,get(measure)), by = i, .SDcols = measure_col]
}
}
fn_agg(DT_train,nonbin_cols,"mean","target")
DT_train
str(DT_train)
p_load(onehot)
#Individual encodings
bin_cols <- lapply(DT_train,nlevels)[lapply(DT_train,nlevels) == 2] %>% names()
encoder <-onehot(DT_train[,..bin_cols])
encoder
encoding_mat <- predict(DT_train[,..bin_cols])
encoding_mat <- predict(encoder,DT_train[,..bin_cols])
encoding_mat
dim(encoding_mat)
str(DT_train)
DT_train[,!..bin-cols]
DT_train[,!..bin_cols]
DT_train <- DT_train[,!..bin_cols]
DT_train <- DT_train[,!..nonbin_cols]
str(DT_train)
as.data.frame(encoding_mat,row.names = F)
dim(encoding_mat)
dim(DT_train)
encoding_mat <- as.data.frame(encoding_mat,row.names = F)
grep("=0",names(encoding_mat),value = T)
setDT(encoding_mat)
grep("=0",names(encoding_mat),value = T)
cols <- grep("=0",names(encoding_mat),value = T)
encoding_mat[,!..cols]
encoding_mat <- encoding_mat[,!..cols]
dim(encoding_mat)
dim(DT_train)
DT_train <- cbind(DT_train,encoding_mat)
str(DT_train)
saveRDS(DT_train, here::here("content","R_scripts_and_functions","porto_seguro","DT_train_encoded.Rds"))
embed_methods <- c("PCA","tSNE","FastICA","DRR") ## define which methods to apply
## apply dimensionality reduction
cols <- c("target","id")
data_emb <- lapply(embed_methods, function(x) embed(train[,!..cols], x))
## apply dimensionality reduction
cols <- c("target","id")
data_emb <- lapply(embed_methods, function(x) embed(train[,!..cols], x))
data_emb <- lapply(embed_methods, function(x) embed(DT_train[,!..cols], x))
if(!require(cutlery)) { devtools::install_github("amit-agni/cutlery"); library(cutlery)}
if(!require(pacman)) { install.packages("pacman"); library(pacman)}
p_load(here,data.table,tidyverse,tictoc,kableExtra,gridExtra,RColorBrewer,DescTools
#,na.tools # For basic NA imputation
#,psych # for phi coefficient of correlation
#,visdat # Missing value visualisation
,dimRed #dimRed
#,mlr
#,caret #stratified sampling only
#,vtreat
)
source(here::here("content","R_scripts_and_functions","porto_seguro","porto_functions.R"))
#saveRDS(DT_train, here::here("content","R_scripts_and_functions","porto_seguro","DT_train_encoded.Rds"))
DT_train <- readRDS(here::here("content","R_scripts_and_functions","porto_seguro","DT_train_encoded.Rds"))
embed_methods <- c(PCA") ## define which methods to apply
embed_methods <- c("PCA") ## define which methods to apply
## apply dimensionality reduction
cols <- c("target","id")
data_emb <- lapply(embed_methods, function(x) embed(DT_train[,!..cols], x))
rm(data_emb)
gc()
embed_methods <- c("FastICA") ## define which methods to apply
gc()
embed_methods <- c("FastICA") ## define which methods to apply
data_emb <- lapply(embed_methods, function(x) embed(DT_train[,!..cols], x))
install.packages('fastICA')
rm(data_emb)
gc()
gc()
embed_methods <- c("FastICA") ## define which methods to apply
data_emb <- lapply(embed_methods, function(x) embed(DT_train[,!..cols], x))
rm(data_emb)
gc()
embed_methods <- c("DRR") ## define which methods to apply
data_emb <- lapply(embed_methods, function(x) embed(DT_train[,!..cols], x))
embed_methods <- c("tSNE") ## define which methods to apply
rm(data_emb)
gc()
embed_methods <- c("tSNE") ## define which methods to apply
data_emb <- lapply(embed_methods, function(x) embed(DT_train[,!..cols], x))
embed_methods <- c("FastICA") ## define which methods to apply
rm(data_emb)
gc()
gc()
data_emb <- lapply(embed_methods, function(x) embed(DT_train[,!..cols], x))
names(data_emb) <- embed_methods
plot_R_NX(data_emb)
plot(data_emb$Isomap)
plot(data_emb$FastICA)
dimRed::dimRedMethodList()
embed_methods <- c("kPCA") ## define which methods to apply
gc()
gc()
embed_methods <- c("kPCA") ## define which methods to apply
data_emb <- lapply(embed_methods, function(x) embed(DT_train[,!..cols], x))
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::insert_image_addin()
blogdown:::serve_site()
blogdown:::insert_image_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::insert_image_addin()
if(!require(pacman)) { install.packages("pacman"); library(pacman)}
install.packages("pacman");
install.packages("pacman")
if(!require(pacman)) { install.packages("pacman"); library(pacman)}
p_load(here,data.table,tidyverse,tictoc)
p_load(here,data.table,tidyverse,tictoc,kableExtra,gridExtra,devtools)
if(!require(cutlery)) { devtools::install_github("amit-agni/cutlery"); library(cutlery)}
p_load(here,data.table,tidyverse,tictoc,kableExtra,gridExtra,devtools,blogdown)
blogdown:::serve_site()
blogdown::install_hugo()
blogdown:::serve_site()
p_load(here,data.table,tidyverse,tictoc,blogdown)
if(!require(pacman)) { install.packages("pacman"); library(pacman)}
p_load(here,data.table,tidyverse,tictoc,blogdown)
install_hugo(version = "0.62.0")
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
blogdown:::serve_site()
?reactiveVal
?reactiveVal
blogdown:::insert_image_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
knitr::opts_chunk$set(echo = TRUE, fig.align='center')
if(!require(cutlery)) { devtools::install_github("amit-agni/cutlery"); library(cutlery)}
if(!require(pacman)) { install.packages("pacman"); library(pacman)}
p_load(here,data.table,tidyverse,tictoc,kableExtra,gridExtra)
p_load(rgdal #reading the shape files, drivers for several formats
,rgeos#
,sf
,rmapshaper
,leaflet
,tmap
,ggmap
)
varPlotCaption <- "© Data Learnings"
ggplot_color_theme <- "turquoise4"
here()
here::here('content\post\data\2020-05-01-geographic-data-and-visualisation-in-r')
obj_rgdal <- rgdal::readOGR(dsn=here::here('content','post','data','2020-05-01-geographic-data-and-visualisation-in-r'),layer="SA2_2016_AUST" )
obj_rgdal <- rgdal::readOGR(dsn=here::here('content','post','data','2020-05-01-geographic-data-and-visualisation-in-r',
'ESRI-Shape-files-1270055001_sa2_2016_aust_shape'),layer="SA2_2016_AUST" )
class(obj_rgdal)
head(obj_rgdal@data)
obj_sf <- sf::st_read(dsn=here::here('content','post','data','2020-05-01-geographic-data-and-visualisation-in-r',
'ESRI-Shape-files-1270055001_sa2_2016_aust_shape'),layer="SA2_2016_AUST")
class(obj_sf)
head(obj_sf,10)
obj_sf <- obj_sf[!st_is_empty(obj_sf),]
#Add the population stored in the Tot_P_P to the obj_sf
populationQLD <- read.csv(here::here('content','post','data','2020-05-01-geographic-data-and-visualisation-in-r',
'2016Census_G01_QLD_SA2.csv'))
blogdown:::serve_site()
blogdown:::serve_site()
knitr::opts_chunk$set(echo = TRUE, fig.align='center', warning = F, error = F)
if(!require(cutlery)) { devtools::install_github("amit-agni/cutlery"); library(cutlery)}
if(!require(pacman)) { install.packages("pacman"); library(pacman)}
p_load(here,data.table,tidyverse,tictoc,kableExtra,gridExtra)
p_load(rgdal #reading the shape files, drivers for several formats
,rgeos#
,sf
,rmapshaper
,leaflet
,tmap
,ggmap
)
varPlotCaption <- "© Data Learnings"
ggplot_color_theme <- "turquoise4"
head(obj_rgdal@data) %>%
kable(format.args = list(decimal.mark = '.', big.mark = ",")) %>%
kable_styling(bootstrap_options = "condensed"
,full_width = FALSE
,position = "left"
,font_size = 10)
blogdown:::serve_site()
blogdown:::insert_image_addin()
head(obj_sf,10)
head(obj_sf[,-"geometry"],10)
class(obj_sf)
which(names(obj_sf,"geometry"))
which(names(obj_sf),"geometry")
which(names(obj_sf) == "geometry")
which(!names(obj_sf) == "geometry")
head(obj_sf[,which(!names(obj_sf) == "geometry")],10)
blogdown:::insert_image_addin()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
blogdown:::serve_site()
install.packages("sf")
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
