---
title: Inferential Statistics Part III – ANOVA
author: Amit Agni
date: '2018-04-07'
slug: inferential-statistics-part-iii-anova
categories:
  - statistics
tags:
  - all posts
  - statistics
robots: index
description: write in quotes
output:
  blogdown::html_page:
    toc: yes
    fig_height: 2
    fig_width: 5
---

<script src="/rmarkdown-libs/kePrint/kePrint.js"></script>

<div id="TOC">
<ul>
<li><a href="#one-way-anova">One-way ANOVA</a></li>
<li><a href="#two-way-anova">Two way ANOVA</a></li>
<li><a href="#summary">Summary</a></li>
</ul>
</div>

<center>
<!-- feature image -->
<p><img src="/post/2018-04-07-inferential-statistics-part-iii-anova_files/tisma-jrdl-1AwGus3QVE4-unsplash.jpg" />
<em>Photo by Tisma Jrdl on Unsplash</em></p>
</center>
<hr>
<p>In the <a href="/inferential-statistics-part-ii-t-test/">previous post</a>, we looked at T-tests to compare the means of one or two samples. The T-tests can still be used for more than samples but there are 2 issues with it :</p>
<ul>
<li>It will be tedious to compare every sample with every other samples</li>
<li>The probability of making Type I error -False Positive (when we reject Null instead of failing to reject) multiples exponentially. The probability of Type I error for a one sample T-test with alpha of 0.05 is 5%. For n number of independent t-tests this error is 1-(1-a)^n</li>
</ul>
<p>To overcome these issues in comparing more than two samples, we use ANOVA to check whether the variability in the sample means is so large that it seems unlikely to be from chance alone?</p>
<p>There are 2 types of ANOVA tests</p>
<ul>
<li>One-way ANOVA is used when there is one independent variable (with more than 2 levels) and one dependent variable</li>
<li>Two-way ANOVA is used when there are more one independent variable (with more than 2 levels each) and one dependent variable</li>
</ul>
<div id="one-way-anova" class="section level3">
<h3>One-way ANOVA</h3>
<ul>
<li>The ANOVA is different from other tests as we have to compute different measures and then use them to calculate the F-score.</li>
<li>The measures that need to be computed for updating the table are given below</li>
</ul>
<center>
<img src="/post/2018-04-07-inferential-statistics-part-iii-anova_files/Oneway%20ANOVA%20measures.jpeg" style="width:50.0%;height:50.0%" />
</center>
<ul>
<li>Total Sum of Squares <span class="math inline">\(SS_{T}\)</span> is the sum of the difference between each value y from the grand mean for N observations <span class="math inline">\(SS_{T} = \sum (y - \bar{y}^2) = \sum y^2 - \frac{(\sum y)^2}{N}\)</span></li>
<li>Sum of Squares Between <span class="math inline">\(SS_{B}\)</span> for k groups, <span class="math inline">\(n_{k}\)</span> observations in group k and <span class="math inline">\(\bar y_{k}\)</span> being the mean of the group k is given by <span class="math inline">\(SS_{B} = \sum n_{k} (\bar y_{k} - \bar {y})^2\)</span></li>
<li>Sum of Squares Within groups <span class="math inline">\(SS_{W} = SS_{T} - SS_{B}\)</span></li>
<li>The degrees of freedom are given by
<ul>
<li><span class="math inline">\(df_{Total} = N - 1\)</span></li>
<li><span class="math inline">\(df_{Between} = k - 1\)</span></li>
<li><span class="math inline">\(df_{Within} = N - k\)</span></li>
</ul></li>
<li>We then calculate the mean square error with the associated degrees of freedom.
<ul>
<li><span class="math inline">\(MS_{B} = \frac {SS_{B}}{df_{Between}}\)</span> measures between-group variability</li>
<li><span class="math inline">\(MS_{W} = \frac {SS_{W}}{df_{Within}}\)</span> measures variability within each of the groups</li>
</ul></li>
<li><p>And finally the F statistic is the ratio <span class="math inline">\(\frac {MS_{B}}{MS_{W}}\)</span></p></li>
<li>When the null hypothesis is true any difference among the sample means are only due to chance and MSB and MSW should be equal.</li>
<li><p>The larger the observed variability in the samples means MSB relative to the within group observations MSW, the larger F will be and strong evidence against the null hypothesis.</p></li>
</ul>
<div id="example" class="section level4">
<h4>Example</h4>
<ul>
<li>The first assumption of ANOVA is that all observations are independent of one another and randomly selected from the population which they represent. We will assume this is satisfied</li>
<li>The second assumption is that the the variance across groups must be almost the same. If we look at all the variables only Sepal.Width seems to be satisfying this condition.</li>
</ul>
<pre class="r"><code>aggregate(x=iris[1:4],by = list(iris$Species), FUN=mean) %&gt;%
kable(format.args = list(decimal.mark = &#39;.&#39;, big.mark = &quot;,&quot;)) %&gt;%
kable_styling(bootstrap_options = &quot;condensed&quot;
            ,full_width = FALSE
            ,position = &quot;center&quot;
            ,font_size = 10)</code></pre>
<table class="table table-condensed" style="font-size: 10px; width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Group.1
</th>
<th style="text-align:right;">
Sepal.Length
</th>
<th style="text-align:right;">
Sepal.Width
</th>
<th style="text-align:right;">
Petal.Length
</th>
<th style="text-align:right;">
Petal.Width
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
setosa
</td>
<td style="text-align:right;">
5.006
</td>
<td style="text-align:right;">
3.428
</td>
<td style="text-align:right;">
1.462
</td>
<td style="text-align:right;">
0.246
</td>
</tr>
<tr>
<td style="text-align:left;">
versicolor
</td>
<td style="text-align:right;">
5.936
</td>
<td style="text-align:right;">
2.770
</td>
<td style="text-align:right;">
4.260
</td>
<td style="text-align:right;">
1.326
</td>
</tr>
<tr>
<td style="text-align:left;">
virginica
</td>
<td style="text-align:right;">
6.588
</td>
<td style="text-align:right;">
2.974
</td>
<td style="text-align:right;">
5.552
</td>
<td style="text-align:right;">
2.026
</td>
</tr>
</tbody>
</table>
<pre class="r"><code>aggregate(x=iris[1:4],by = list(iris$Species), FUN=sd) %&gt;%
kable(format.args = list(decimal.mark = &#39;.&#39;, big.mark = &quot;,&quot;)) %&gt;%
kable_styling(bootstrap_options = &quot;condensed&quot;
            ,full_width = FALSE
            ,position = &quot;center&quot;
            ,font_size = 10)</code></pre>
<table class="table table-condensed" style="font-size: 10px; width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Group.1
</th>
<th style="text-align:right;">
Sepal.Length
</th>
<th style="text-align:right;">
Sepal.Width
</th>
<th style="text-align:right;">
Petal.Length
</th>
<th style="text-align:right;">
Petal.Width
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
setosa
</td>
<td style="text-align:right;">
0.3524897
</td>
<td style="text-align:right;">
0.3790644
</td>
<td style="text-align:right;">
0.1736640
</td>
<td style="text-align:right;">
0.1053856
</td>
</tr>
<tr>
<td style="text-align:left;">
versicolor
</td>
<td style="text-align:right;">
0.5161711
</td>
<td style="text-align:right;">
0.3137983
</td>
<td style="text-align:right;">
0.4699110
</td>
<td style="text-align:right;">
0.1977527
</td>
</tr>
<tr>
<td style="text-align:left;">
virginica
</td>
<td style="text-align:right;">
0.6358796
</td>
<td style="text-align:right;">
0.3224966
</td>
<td style="text-align:right;">
0.5518947
</td>
<td style="text-align:right;">
0.2746501
</td>
</tr>
</tbody>
</table>
<ul>
<li>The third assumption the distribution should be approximately normal at each factor level. This is also satisfied, there are few outliers which we will ignore</li>
</ul>
<pre class="r"><code>ggplot(data=iris[,c(2,5)]) +
 geom_density(aes(x=Sepal.Width,fill=Species)) +
 facet_wrap(~Species) +
  labs(caption = varPlotCaption) +
  theme_darklightmix(color_theme = ggplot_color_theme) +
  scale_fill_brewer(palette=&quot;Set3&quot;)</code></pre>
<p><img src="/post/2018-04-07-inferential-statistics-part-iii-anova_files/figure-html/unnamed-chunk-2-1.png" width="480" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(data=iris[,c(2,5)]) +
 geom_boxplot(aes(y=Sepal.Width,fill=Species)) +
 facet_wrap(~Species) +
  labs(caption = varPlotCaption) +
  theme_darklightmix(color_theme = ggplot_color_theme)  +
  scale_fill_brewer(palette=&quot;Set3&quot;)</code></pre>
<p><img src="/post/2018-04-07-inferential-statistics-part-iii-anova_files/figure-html/unnamed-chunk-2-2.png" width="480" style="display: block; margin: auto;" /></p>
<ul>
<li>The ANOVA test gives us a F value of 49.16 which is much higher than the F-critical value of 3.06.</li>
<li>And the probability that this high value could have occurred by chance is very less and hence we can reject the null hypothesis that the mean of all the three groups are equal.</li>
</ul>
<pre class="r"><code>#get the critical value of the F statistic
qf(df1=2,df2=147, p= 0.05,lower.tail=FALSE)</code></pre>
<pre><code>## [1] 3.057621</code></pre>
<pre class="r"><code># 3.057621

summary(aov(iris$Sepal.Width ~ iris$Species))</code></pre>
<pre><code>##               Df Sum Sq Mean Sq F value              Pr(&gt;F)    
## iris$Species   2  11.35   5.672   49.16 &lt;0.0000000000000002 ***
## Residuals    147  16.96   0.115                                
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<ul>
<li><strong>Since we got a significant p-value then we can run post-hoc analysis to find which group means are different from each other</strong></li>
<li><strong>Couple of methods of post-hoc analysis are Tukey Honest Significant Differences and Bonerroni post-hoc analysis</strong></li>
<li>We can see that the probability (p-adj) for the means of versicolor-setosa and<br />
virginica-setosa to be equal is very very less.</li>
</ul>
<pre class="r"><code>TukeyHSD(aov(iris$Sepal.Width ~ iris$Species))</code></pre>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = iris$Sepal.Width ~ iris$Species)
## 
## $`iris$Species`
##                        diff         lwr        upr     p adj
## versicolor-setosa    -0.658 -0.81885528 -0.4971447 0.0000000
## virginica-setosa     -0.454 -0.61485528 -0.2931447 0.0000000
## virginica-versicolor  0.204  0.04314472  0.3648553 0.0087802</code></pre>
</div>
</div>
<div id="two-way-anova" class="section level3">
<h3>Two way ANOVA</h3>
<ul>
<li>In one-way ANOVA, we had one dependent variable and one independent variable.</li>
<li>In two way ANOVA we can have more than one independent variables, so we need to calculate a ratio that measures not only the variation between the dependent and independent variables, but also the interaction between the two independent variables.</li>
</ul>
<div id="example-1" class="section level4">
<h4>Example</h4>
<ul>
<li>We will use the ToothGrowth dataset in R.
<ul>
<li>The dataset has the observations of the tooth growth in 60 guinea pigs which were administered three doses of Vitamin C (0.1, 1 and 2 mg/day).</li>
<li>It also has the delivery methods using which these doses were administered – OJ (Orange Juice) and VC (ascorbic acid which is a form Vitamin C)</li>
</ul></li>
<li>For two-way ANOVA, we have three null hypotheses:
<ol style="list-style-type: decimal">
<li>In the population, the means for the rows equal each other. In the example above, we would say that the mean for the delivery methods are equal</li>
<li>In the population, the means for the columns equal each other. In the example above, we would say that the means for the three dosages are equal.</li>
<li>In the population, the null hypothesis would be that there is no interaction between the two variables. In the example above, we would say that there is no interaction between the delivery method and amount of dosage, or that all
effects equal 0.</li>
</ol></li>
<li>A quick look at the boxplot indicates there are couple of outliers and we will ignore them as before. It also indicates that Dose 2 resulted in longer tooth growth and so also Dose 1 given in Orange Juice</li>
</ul>
<pre class="r"><code>str(ToothGrowth)</code></pre>
<pre><code>## &#39;data.frame&#39;:    60 obs. of  3 variables:
##  $ len : num  4.2 11.5 7.3 5.8 6.4 10 11.2 11.2 5.2 7 ...
##  $ supp: Factor w/ 2 levels &quot;OJ&quot;,&quot;VC&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##  $ dose: num  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 ...</code></pre>
<pre class="r"><code>ToothGrowth$dose &lt;- as.factor(ToothGrowth$dose)

ggplot(data=ToothGrowth) +
 geom_boxplot(aes(x=dose,y=len,fill=supp)) + 
   labs(subtitle=&quot;Box plot of tooth growth for various doses&quot;
   ,caption = varPlotCaption) +
   theme_darklightmix(color_theme = ggplot_color_theme) +
   scale_fill_brewer(palette = &quot;Set3&quot;)</code></pre>
<p><img src="/post/2018-04-07-inferential-statistics-part-iii-anova_files/figure-html/unnamed-chunk-5-1.png" width="480" style="display: block; margin: auto;" /></p>
<pre class="r"><code>attach(ToothGrowth)
aggregate(ToothGrowth$len, by=list(supp,dose),
 FUN=mean,
 na.rm=TRUE,
 drop=TRUE) %&gt;%
spread(key=Group.2, value=x)
 
aggregate(ToothGrowth$len, by=list(supp),FUN=mean, na.rm=TRUE)
aggregate(ToothGrowth$len, by=list(dose),FUN=mean, na.rm=TRUE)</code></pre>
<ul>
<li>Lets run the ANOVA test. We can see that F-values of dosage and the method of delivery are higher than the F critical values with alpha 0.05. The low probability indicating that this did not occur by some random chance but they are statistically significant.</li>
<li>If we had to choose a lower critical value of say 0.02 then the interaction would not have been statistically significant and we could have concluded that there is no interaction between the dosage and the delivery method.</li>
</ul>
<pre class="r"><code>summary(aov(len ~ supp + dose + supp * dose
            ,data=ToothGrowth))</code></pre>
<pre><code>##             Df Sum Sq Mean Sq F value               Pr(&gt;F)    
## supp         1  205.4   205.4  15.572             0.000231 ***
## dose         2 2426.4  1213.2  92.000 &lt; 0.0000000000000002 ***
## supp:dose    2  108.3    54.2   4.107             0.021860 *  
## Residuals   54  712.1    13.2                                 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>#get the critical value of the F statistic
qf(df1=1,df2=54, p= 0.05,lower.tail=FALSE)</code></pre>
<pre><code>## [1] 4.019541</code></pre>
<pre class="r"><code>qf(df1=2,df2=54, p= 0.05,lower.tail=FALSE)</code></pre>
<pre><code>## [1] 3.168246</code></pre>
<ul>
<li>The Tukey HSD is given below.
<ul>
<li>The probability of mean tooth growth due to VC (Vit C) and OJ (Orange Juice) being equal is very less (0.0002312)</li>
<li>Similarly probability of the mean of tooth growth (for different doses) being equal is also very low</li>
<li>If we look at the interaction terms
<ul>
<li><strong>VC:1-OJ:0.5</strong> has a high probability of 26% which indicates a 1mg dose given through absorbic acid (VC:1) has same effect as that of 0.5mg dose given through orange juice (OJ:0.5)</li>
<li><strong>VC:2-OJ:2</strong> indicates the ………. TBD</li>
</ul></li>
</ul></li>
</ul>
<p>The interaction between OJ2-VC2 is not statistically significant with 100% probability that this occurred by random chance ? So also, VC1-OJ5 (26%), OJ2-OJ1 (32%) and VC2-OJ1(29%).</p>
<p>Whereas the other interactions are statistically significant and need further investigation ?</p>
<pre class="r"><code>TukeyHSD(aov(len ~ supp + dose + supp * dose
             ,data=ToothGrowth))</code></pre>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = len ~ supp + dose + supp * dose, data = ToothGrowth)
## 
## $supp
##       diff       lwr       upr     p adj
## VC-OJ -3.7 -5.579828 -1.820172 0.0002312
## 
## $dose
##         diff       lwr       upr     p adj
## 1-0.5  9.130  6.362488 11.897512 0.0000000
## 2-0.5 15.495 12.727488 18.262512 0.0000000
## 2-1    6.365  3.597488  9.132512 0.0000027
## 
## $`supp:dose`
##                diff        lwr        upr     p adj
## VC:0.5-OJ:0.5 -5.25 -10.048124 -0.4518762 0.0242521
## OJ:1-OJ:0.5    9.47   4.671876 14.2681238 0.0000046
## VC:1-OJ:0.5    3.54  -1.258124  8.3381238 0.2640208
## OJ:2-OJ:0.5   12.83   8.031876 17.6281238 0.0000000
## VC:2-OJ:0.5   12.91   8.111876 17.7081238 0.0000000
## OJ:1-VC:0.5   14.72   9.921876 19.5181238 0.0000000
## VC:1-VC:0.5    8.79   3.991876 13.5881238 0.0000210
## OJ:2-VC:0.5   18.08  13.281876 22.8781238 0.0000000
## VC:2-VC:0.5   18.16  13.361876 22.9581238 0.0000000
## VC:1-OJ:1     -5.93 -10.728124 -1.1318762 0.0073930
## OJ:2-OJ:1      3.36  -1.438124  8.1581238 0.3187361
## VC:2-OJ:1      3.44  -1.358124  8.2381238 0.2936430
## OJ:2-VC:1      9.29   4.491876 14.0881238 0.0000069
## VC:2-VC:1      9.37   4.571876 14.1681238 0.0000058
## VC:2-OJ:2      0.08  -4.718124  4.8781238 1.0000000</code></pre>
</div>
</div>
<div id="summary" class="section level3">
<h3>Summary</h3>
<ul>
<li><strong>The null hypothesis for ANOVA is that the mean of the dependent variable is the same for all groups</strong></li>
<li>Further learning
<ul>
<li>Balanced vs Unbalanced design experiments for ANOVA</li>
<li>Compare the results with <a href="http://www.sthda.com/english/wiki/two-way-anova-test-in-r" class="uri">http://www.sthda.com/english/wiki/two-way-anova-test-in-r</a></li>
</ul></li>
</ul>
</div>
